{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49abf0a6",
   "metadata": {},
   "source": [
    "### Problem Statement "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5834aaaf",
   "metadata": {},
   "source": [
    "Objective: Develop a real or fake news predictive system that can accurately classify news as either real or fake based on their content.\n",
    "\n",
    "Background:\n",
    "- With the proliferation of digital media and social networks, fake news has become a significant concern, leading to misinformation, manipulation, and confusion among the public.\n",
    "- The ability to distinguish between real and fake news is crucial for maintaining the integrity of information and promoting informed decision-making.\n",
    "\n",
    "Dataset:\n",
    "- The problem statement typically involves using a labeled dataset containing examples of both real and fake news articles.\n",
    "\n",
    "Key Tasks:\n",
    "1. Data Preprocessing:\n",
    "   - Clean and preprocess the text data, including tasks such as tokenization, stop word removal, and stemming/lemmatization.\n",
    "   - Convert text data into a format suitable for machine learning algorithms (e.g., numerical vectors using techniques like TF-IDF).\n",
    "   \n",
    "3. Model Development:\n",
    "   - Choose appropriate machine learning algorithms for classification, such as logistic regression, naive bayes\n",
    "   \n",
    "   - Train and evaluate multiple models using appropriate performance metrics (e.g., accuracy, classification report & confusion matrix) on validation or test data.\n",
    "  \n",
    "4. Model Evaluation:\n",
    "   - Assess the performance of the trained models using cross-validation or a separate test dataset.\n",
    "   - Fine-tune hyperparameters and model parameters to improve performance if necessary.\n",
    "\n",
    "Success Criteria:\n",
    "- The success of the real or fake news identification system is determined by its ability to accurately classify news articles and minimize misclassifications (e.g., false positives and false negatives)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e76bf22c",
   "metadata": {},
   "source": [
    "1: Fake news # labels for real or fake articles \n",
    "0: real News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32be88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing in the Problem "
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf26e79d",
   "metadata": {},
   "source": [
    "# About the dataset\n",
    "title: title of the article.\n",
    "news_url: URL of the article.\n",
    "source domain: web domain where article was posted.\n",
    "tweet_num: number of retweets for this article.\n",
    "real: label column, where 1 is real and 0 is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f4d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies/packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re  # regular expressions\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "#natrural language tool kit for removing stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer  # stemming removes the suffixes & prefixes\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # we are converting text into numeric feature vectors \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbc8a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # download all the stop words present\n",
    "import nltk  \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26301eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english')) # language is english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7082e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will remove all these words from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2015329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecf7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset to a pandas dataframe\n",
    "news_dataset = pd.read_csv('FakeNewsNet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1083c12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23196, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.shape # so we have 23196 news articles in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c7b8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'news_url', 'source_domain', 'tweet_num', 'real'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.columns # data column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a5f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.rename(columns={'real':'label'}, inplace = True )# for my better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8faa35c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>news_url</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>tweet_num</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kandi Burruss Explodes Over Rape Accusation on...</td>\n",
       "      <td>http://toofab.com/2017/05/08/real-housewives-a...</td>\n",
       "      <td>toofab.com</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People's Choice Awards 2018: The best red carp...</td>\n",
       "      <td>https://www.today.com/style/see-people-s-choic...</td>\n",
       "      <td>www.today.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sophia Bush Sends Sweet Birthday Message to 'O...</td>\n",
       "      <td>https://www.etonline.com/news/220806_sophia_bu...</td>\n",
       "      <td>www.etonline.com</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Colombian singer Maluma sparks rumours of inap...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-33655...</td>\n",
       "      <td>www.dailymail.co.uk</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gossip Girl 10 Years Later: How Upper East Sid...</td>\n",
       "      <td>https://www.zerchoo.com/entertainment/gossip-g...</td>\n",
       "      <td>www.zerchoo.com</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Kandi Burruss Explodes Over Rape Accusation on...   \n",
       "1  People's Choice Awards 2018: The best red carp...   \n",
       "2  Sophia Bush Sends Sweet Birthday Message to 'O...   \n",
       "3  Colombian singer Maluma sparks rumours of inap...   \n",
       "4  Gossip Girl 10 Years Later: How Upper East Sid...   \n",
       "\n",
       "                                            news_url        source_domain  \\\n",
       "0  http://toofab.com/2017/05/08/real-housewives-a...           toofab.com   \n",
       "1  https://www.today.com/style/see-people-s-choic...        www.today.com   \n",
       "2  https://www.etonline.com/news/220806_sophia_bu...     www.etonline.com   \n",
       "3  https://www.dailymail.co.uk/news/article-33655...  www.dailymail.co.uk   \n",
       "4  https://www.zerchoo.com/entertainment/gossip-g...      www.zerchoo.com   \n",
       "\n",
       "   tweet_num  label  \n",
       "0         42      1  \n",
       "1          0      1  \n",
       "2         63      1  \n",
       "3         20      1  \n",
       "4         38      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head() # first five rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c01d2c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>news_url</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>tweet_num</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6327</th>\n",
       "      <td>Robert Pattinson Is Really Crushing On Margot ...</td>\n",
       "      <td>okmagazine.com/photos/robert-pattinson-crush-o...</td>\n",
       "      <td>okmagazine.com</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17802</th>\n",
       "      <td>Law &amp; Order: Special Victims Unit (season 1)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Law_%26_Order:_S...</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20965</th>\n",
       "      <td>19 Years In The Life Of Jessica Simpson</td>\n",
       "      <td>www.businessinsider.com/jessica-simpson-2011-11</td>\n",
       "      <td>www.businessinsider.com</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21124</th>\n",
       "      <td>Jennifer Aniston And Katy Perry Not Fighting O...</td>\n",
       "      <td>www.inquisitr.com/4826781/jennifer-aniston-and...</td>\n",
       "      <td>www.inquisitr.com</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19156</th>\n",
       "      <td>Kim Kardashian shares adorable video of Saint ...</td>\n",
       "      <td>www.mirror.co.uk/3am/celebrity-news/kim-kardas...</td>\n",
       "      <td>www.mirror.co.uk</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "6327   Robert Pattinson Is Really Crushing On Margot ...   \n",
       "17802       Law & Order: Special Victims Unit (season 1)   \n",
       "20965            19 Years In The Life Of Jessica Simpson   \n",
       "21124  Jennifer Aniston And Katy Perry Not Fighting O...   \n",
       "19156  Kim Kardashian shares adorable video of Saint ...   \n",
       "\n",
       "                                                news_url  \\\n",
       "6327   okmagazine.com/photos/robert-pattinson-crush-o...   \n",
       "17802  https://en.wikipedia.org/wiki/Law_%26_Order:_S...   \n",
       "20965    www.businessinsider.com/jessica-simpson-2011-11   \n",
       "21124  www.inquisitr.com/4826781/jennifer-aniston-and...   \n",
       "19156  www.mirror.co.uk/3am/celebrity-news/kim-kardas...   \n",
       "\n",
       "                 source_domain  tweet_num  label  \n",
       "6327            okmagazine.com         10      0  \n",
       "17802         en.wikipedia.org         32      1  \n",
       "20965  www.businessinsider.com         34      0  \n",
       "21124        www.inquisitr.com         14      0  \n",
       "19156         www.mirror.co.uk          2      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.sample(5)# random 5 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce60591e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title              0\n",
       "news_url         330\n",
       "source_domain    330\n",
       "tweet_num          0\n",
       "label              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the missing values\n",
    "news_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd677ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 330 news_url & source_domains are missing here, but we have enough dataset to train our model we can either drop it or impute it\n",
    "# we can impute it with null strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c58fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing null values with empty/null strings \n",
    "news_dataset = news_dataset.fillna(' ') # this is a null string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82895397",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.751897\n",
       "0    0.248103\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset['label'].value_counts(normalize = True)# so we have the balanced, proportionate data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b45d4b90",
   "metadata": {},
   "source": [
    "Weighted Loss Function: Adjusting the class weights in the classifier's loss function can give more importance to minority class samples during training. This allows the classifier to learn from the minority class examples effectively without the need for oversampling or undersampling.\n",
    "\n",
    "Ensemble Methods: Using ensemble methods like bagging or boosting with resampling techniques can help improve classification performance on imbalanced datasets.\n",
    "\n",
    "while oversampling and undersampling techniques can be applied to address class imbalance in text classification, they may not be directly applicable to text data due to the complexities involved in preserving semantic meaning and coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3dd8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is a process of reducing a word to base form\n",
    "# example:\n",
    "# actor, actress, acting--> act\n",
    "# in vectorization we will convert these words into the numeric feature vectores which will be fed into the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96734fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem  = PorterStemmer() # created the instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d5e40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a user defined function for stemming\n",
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)# regex, removed everythiung other than letters A-Za-z\n",
    "    stemmed_content = stemmed_content.lower() # lowercase\n",
    "    stemmed_content = stemmed_content.split() # splited text into list of words strings\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2047ec07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news = news_dataset['title'].apply(stemming) # applying the stemming to the title column, because that is the content which we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76157559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    kandi burruss explod rape accus real housew at...\n",
       "1               peopl choic award best red carpet look\n",
       "2    sophia bush send sweet birthday messag one tre...\n",
       "3    colombian singer maluma spark rumour inappropr...\n",
       "4    gossip girl year later upper east sider shock ...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c0ac13f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kandi burruss explod rape accus real housew atlanta reunion video'\n",
      " 'peopl choic award best red carpet look'\n",
      " 'sophia bush send sweet birthday messag one tree hill co star hilari burton breyton eva'\n",
      " ... 'jessica chastain recal moment mother boyfriend slap kick genit'\n",
      " 'tristan thompson feel dump khlo kardashian refus let move la home exclus'\n",
      " 'kelli clarkson perform medley kendrick lamar humbl hit billboard music award']\n",
      "[1 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# separating the data # we have text in X variable & label in y variable\n",
    "X = news.values\n",
    "y = news_dataset['label'].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30c29d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23196,) (23196,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "864c4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting textual data into numerical data\n",
    "vectorizer = TfidfVectorizer()# defined object \n",
    "vectorizer.fit(X)\n",
    "X = vectorizer.transform(X) # to vectorize the text data into numeric feature vectors, first it will learn than transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b247d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf\n",
    "# term frequency: no of time a word is repeating in a document\n",
    "# inverse document frequency: it provides heighest weightage is given to less occuring but more meaningful/significant word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7c4ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11959)\t0.22178153496429145\n",
      "  (0, 9287)\t0.2664785410247072\n",
      "  (0, 9020)\t0.2424928018856143\n",
      "  (0, 8975)\t0.31659115198843385\n",
      "  (0, 5895)\t0.40600285234371225\n",
      "  (0, 5246)\t0.2737247638213213\n",
      "  (0, 3739)\t0.3720213547026605\n",
      "  (0, 1524)\t0.40600285234371225\n",
      "  (0, 576)\t0.3220576151506555\n",
      "  (0, 53)\t0.2721149519149911\n",
      "  (1, 9079)\t0.387076508000102\n",
      "  (1, 8315)\t0.4067948402953306\n",
      "  (1, 6573)\t0.3420111696667716\n",
      "  (1, 1975)\t0.43378159796649224\n",
      "  (1, 1710)\t0.3978429114527965\n",
      "  (1, 986)\t0.34762560771735407\n",
      "  (1, 643)\t0.31690546637483147\n",
      "  (2, 11479)\t0.3036563626822405\n",
      "  (2, 10939)\t0.2324931756527281\n",
      "  (2, 10595)\t0.148591495027414\n",
      "  (2, 10407)\t0.2836383240767164\n",
      "  (2, 9880)\t0.2644920245799396\n",
      "  (2, 7939)\t0.20215096479825698\n",
      "  (2, 7117)\t0.23353803309354323\n",
      "  (2, 5104)\t0.24183607544147448\n",
      "  :\t:\n",
      "  (23193, 1894)\t0.3741692367316018\n",
      "  (23193, 1278)\t0.2589163123449227\n",
      "  (23194, 11508)\t0.2849153309895167\n",
      "  (23194, 11212)\t0.2829354407102931\n",
      "  (23194, 9115)\t0.334043323633429\n",
      "  (23194, 7432)\t0.29588976326311933\n",
      "  (23194, 6401)\t0.323662596407589\n",
      "  (23194, 6190)\t0.27688531718550113\n",
      "  (23194, 6026)\t0.3017978396551759\n",
      "  (23194, 5908)\t0.1834167295260476\n",
      "  (23194, 5165)\t0.27660251753146414\n",
      "  (23194, 3907)\t0.28229376984976123\n",
      "  (23194, 3706)\t0.25878556482606213\n",
      "  (23194, 3306)\t0.33214426812663195\n",
      "  (23195, 8334)\t0.253789650406438\n",
      "  (23195, 7497)\t0.23055110772502885\n",
      "  (23195, 7047)\t0.37627637539628633\n",
      "  (23195, 6229)\t0.32124439064667815\n",
      "  (23195, 5984)\t0.3573484667755994\n",
      "  (23195, 5968)\t0.24909051401761317\n",
      "  (23195, 5285)\t0.4105590065426586\n",
      "  (23195, 5125)\t0.2765555866595626\n",
      "  (23195, 2063)\t0.2939864729031333\n",
      "  (23195, 1041)\t0.2790174124430189\n",
      "  (23195, 643)\t0.19644507356321092\n"
     ]
    }
   ],
   "source": [
    "print(X) # numeric feature vectors easily machine can understand text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "013d6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting data set to training & test data for model traing & evaluation\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 1, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a261f2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model \n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train) # it will plot the sigmoid graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75f0493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracyscore : 86.0\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.87      0.65      2740\n",
      "           1       0.97      0.86      0.91     15816\n",
      "\n",
      "    accuracy                           0.86     18556\n",
      "   macro avg       0.75      0.86      0.78     18556\n",
      "weighted avg       0.91      0.86      0.87     18556\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 2384   356]\n",
      " [ 2220 13596]]\n"
     ]
    }
   ],
   "source": [
    "# evaluation on training data\n",
    "y_pred_train= logreg.predict(X_train)\n",
    "print('Accuracyscore :', round(accuracy_score(y_pred_train,y_train),2)*100) \n",
    "print('Classification Report :\\n', classification_report(y_pred_train,y_train)) \n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_pred_train,y_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f852258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracyscore : 83.0\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.77      0.56       660\n",
      "           1       0.96      0.84      0.89      3980\n",
      "\n",
      "    accuracy                           0.83      4640\n",
      "   macro avg       0.70      0.80      0.73      4640\n",
      "weighted avg       0.88      0.83      0.85      4640\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 507  153]\n",
      " [ 644 3336]]\n"
     ]
    }
   ],
   "source": [
    "# evaluation on testing data\n",
    "y_pred_test= logreg.predict(X_test)\n",
    "print('Accuracyscore :', round(accuracy_score(y_pred_test,y_test),2)*100) \n",
    "print('Classification Report :\\n', classification_report(y_pred_test,y_test)) \n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_pred_test,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "728923a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# though we dont need regularization as there is just reduction of 3% from training to testing accuracy \n",
    "# but just to check what happens when we are doing regularization & using grid serach cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53d821e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':[1,2,3,4,10], 'penalty': ['l1','l2','elasticnet'], 'max_iter':[100,200,300]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3461a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "Model2 = GridSearchCV(estimator = logreg, param_grid = parameters, cv = 5, scoring = 'accuracy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7eba794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "150 fits failed out of a total of 225.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\navjo\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.83299182        nan        nan 0.83299182        nan\n",
      "        nan 0.83299182        nan        nan 0.83697983        nan\n",
      "        nan 0.83697983        nan        nan 0.83697983        nan\n",
      "        nan 0.83768054        nan        nan 0.83768054        nan\n",
      "        nan 0.83768054        nan        nan 0.83908161        nan\n",
      "        nan 0.8391355         nan        nan 0.8391355         nan\n",
      "        nan 0.83304604        nan        nan 0.83309993        nan\n",
      "        nan 0.83309993        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [1, 2, 3, 4, 10], &#x27;max_iter&#x27;: [100, 200, 300],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [1, 2, 3, 4, 10], &#x27;max_iter&#x27;: [100, 200, 300],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [1, 2, 3, 4, 10], 'max_iter': [100, 200, 300],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2ed53c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracyscore : 83.0\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61       818\n",
      "           1       0.94      0.86      0.89      3822\n",
      "\n",
      "    accuracy                           0.83      4640\n",
      "   macro avg       0.73      0.79      0.75      4640\n",
      "weighted avg       0.86      0.83      0.84      4640\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 599  219]\n",
      " [ 552 3270]]\n"
     ]
    }
   ],
   "source": [
    "# evaluation on testing data\n",
    "y_pred_test= Model2.predict(X_test)\n",
    "print('Accuracyscore :', round(accuracy_score(y_pred_test,y_test),2)*100) \n",
    "print('Classification Report :\\n', classification_report(y_pred_test,y_test)) \n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_pred_test,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73c94ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can see evening using Hyper parameters our accuracy is same 83%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644af586",
   "metadata": {},
   "source": [
    "## Making Predictive System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84b0c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "The news is fake\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[0]\n",
    "# we are just taking one instance from the X-test data & saving it as new data to make the prediction on ot whether its a real/fake news\n",
    "\n",
    "prediction = logreg.predict(X_new)\n",
    "print(prediction)\n",
    "\n",
    "if (prediction[0] == 0):\n",
    "    print('The news is real')\n",
    "else:\n",
    "    print('The news is fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5b2d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# to reconfirm the label we can see it \n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ba58a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_test[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939914e2",
   "metadata": {},
   "source": [
    "## Another model, Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9846a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19d7ae4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = BernoulliNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24a04b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8189655172413793\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.61      1151\n",
      "           1       0.87      0.90      0.88      3489\n",
      "\n",
      "    accuracy                           0.82      4640\n",
      "   macro avg       0.76      0.74      0.75      4640\n",
      "weighted avg       0.81      0.82      0.81      4640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae06a5",
   "metadata": {},
   "source": [
    "## Making Predictive Model with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44b1d038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "The news is fake\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[0]\n",
    "\n",
    "prediction = classifier.predict(X_new)\n",
    "print(prediction)\n",
    "\n",
    "if (prediction[0] == 0):\n",
    "    print('The news is real')\n",
    "else:\n",
    "    print('The news is fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the models are predicting correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
